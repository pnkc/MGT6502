---
title: "MGTXXX Business Analytics"
author: "Sridhar Narasimhan"
subtitle: Text analytics with AirBnb data
date: '`r Sys.Date()`'
output:
  html_document:
    toc: yes
  html_notebook:
    highlight: tango
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    fig_height: 2
    fig_width: 2
    keep_tex: yes
    latex_engine: xelatex
    toc: yes
  word_document:
    toc: no
---

# Set working directoryLoad libraries
Let us start by installing and loading tidyr package.

```{r}
#install.packages("here")

#library(here)
#here("data", "file_i_want.csv")

```

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
#For each .Rmd file we need to set the directory it is operating under so that we can reference other files in different locations. My project location is '/Users/bdcoe/Documents/Data_KB/GT/2017_Spring/BA/r_workspace/' and I have '02_data_wrangling' folder under it with R files and CSV files. So, I am setting my working directory to '/Users/bdcoe/Documents/Data_KB/GT/2017_Spring/BA/r_workspace/02_data_wrangling/'
#knitr::opts_knit$set(root.dir = '~/data/Dropbox (GaTech)/teaching/2018-Spring/online/airbnb/losAngles')
#getwd()

# The easiest way to get tidyr is to install the whole tidyverse:
#install.packages("knitr", repos="http://cran.rstudio.com/")
#install.packages("GGally", repos="http://cran.rstudio.com/")

# Alternatively, install just tidyr:
#install.packages("tidyr")

# Or the the development version from GitHub:
# install.packages("devtools")
#devtools::install_github("tidyverse/tidyr")

#install.packages("wordcloud")

#After installing, we need to call the package to make it available to R
library(readr)
library(stargazer)
library(knitr)
library(dplyr)
#library(GGally)
#library("psych")
library(ggplot2)
library(stringr)
library("ggExtra")
library(psych)



library(dplyr)
library(tidyr)
library(purrr)
library(readr)
#install.packages("topicmodels")
library(tidytext)
library(widyr)
library(ggraph)
library(igraph)
library(tm)
library(topicmodels)

library(wordcloud)
library(reshape2)
library("ldatuning")

#Stemming
#https://github.com/juliasilge/tidytext/issues/17
library(SnowballC)

```   


# Amazon Reviews

Data format:
product/productId: B001E4KFG0
review/userId: A3SGXH7AUHU8GW
review/profileName: delmartian
review/helpfulness: 1/1
review/score: 5.0
review/time: 1303862400
review/summary: Good Quality Dog Food
review/text: I have bought several of the Vitality canned dog food products and have
found them all to be of good quality. The product looks more like a stew than a
processed meat and it smells better. My Labrador is finicky and she appreciates this
product better than most.


URL:
http://snap.stanford.edu/data/web-FineFoods.html

Citation:
J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013.

```{r}
#Stemming




wordStem(c('taste','tasted','tasteful','tastefully','tastes','tasting'), language = "english")



```


```{r}


amazon_reviews_full <- read_tsv("foods.txt",
                           col_names = FALSE
                           #delim = "",
                           #n_max = 24
                           )

#View(head(amazon_reviews_full, 1000))


amazon_reviews <- amazon_reviews_full %>% 
                  #head(1000) %>% 
                  separate(col = X1, 
                           into = c("head", "value"),
                           sep = ": ")
                  #mutate(seq_num = row_number())

# > head(amazon_reviews)
# # A tibble: 6 x 2
#   head        value         
#   <chr>       <chr>         
# 1 productId   B001E4KFG0    
# 2 userId      A3SGXH7AUHU8GW
# 3 profileName delmartian    
# 4 helpfulness 1/1           
# 5 score       5.0           
# 6 time        1303862400    

                    

review <- data.frame(rev_id = 1:nrow(filter(amazon_reviews, head == "productId")),
                     productId = filter(amazon_reviews, head == "productId")$value,
                     userId    = filter(amazon_reviews, head == "userId")$value,
                     rating     = as.numeric(filter(amazon_reviews, head == "score")$value), 
                     text      = filter(amazon_reviews, head == "text")$value, 
                     time      = as.numeric(filter(amazon_reviews, head == "time")$value), 
                     stringsAsFactors = FALSE)

View(head(review,1000))

```


## Tidy text
Clean up text so that we can get it ready for analysis

```{r}
#Remove stop words

tidy_amzn <- review %>%
              unnest_tokens(word, text) %>% 
              anti_join(stop_words) %>% 
              filter(word != "br") %>% #HTML tag <br /><br /> results in the word "br"
              mutate(word = wordStem(word))



#View(head(tidy_amzn,1000))
#glimpse(tidy_amzn)
# $ rev_id    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...
# $ productId <chr> "B001E4KFG0", "B001E4KFG0", "B001E4KFG0", "B001E4KFG0", "B001E4KFG0", "B001E4KFG0...
# $ userId    <chr> "A3SGXH7AUHU8GW", "A3SGXH7AUHU8GW", "A3SGXH7AUHU8GW", "A3SGXH7AUHU8GW", "A3SGXH7A...
# $ score     <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
# $ time      <dbl> 1303862400, 1303862400, 1303862400, 1303862400, 1303862400, 1303862400, 130386240...
# $ word      <chr> "bought", "vitality", "canned", "dog", "food", "products", "found", "quality", "p...

tidy_amzn

head(tidy_amzn)

```

## Word count analysis

```{r}
tidy_amzn %>% 
count(word, sort = TRUE) %>% 
  slice(1:10)
```


```{r}
tidy_amzn %>%
  count(word, sort = TRUE) %>%
  filter(n > 100000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```

## Word cloud

```{r}
tidy_amzn %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```



## Sentiment analysis

```{r}

#The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating #negative sentiment and positive scores indicating positive sentiment. 

# > get_sentiments("afinn")
# # A tibble: 2,476 x 2
#    word       score
#    <chr>      <int>
#  1 abandon       -2
#  2 abandoned     -2
#  3 abandons      -2
#  4 abducted      -2
#  5 abduction     -2
#  6 abductions    -2
#  7 abhor         -3
#  8 abhorred      -3
#  9 abhorrent     -3
# 10 abhors        -3


tidy_amzn_sentiment <- tidy_amzn %>%
                          inner_join(get_sentiments("afinn"), by = "word")
                       


#Now get average sentiment score for each productId to plot rating vs. avg_score
tidy_amzn_sentiment_prod <- tidy_amzn_sentiment %>% 
                            group_by(productId) %>% 
                              summarise(avg_score=mean(score),
                                        sum_score=sum(score),
                                        avg_rating = mean(rating))
    

```

```{r}
ggplot(tidy_amzn_sentiment_prod, aes(x=avg_score, y=avg_rating)) +
    geom_point(shape=1) +    # Use hollow circles
    geom_smooth(method=lm,   # Add linear regression line
                se=TRUE)    # Don't add shaded confidence region


```
## Topic Modelling

Latent Dirichlet allocation (LDA) is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”
Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

Right now our data frame word_counts is in a tidy form, with one-term-per-document-per-row, but the topicmodels package requires a DocumentTermMatrix. As described in Chapter 5.2, we can cast a one-token-per-row table into a DocumentTermMatrix with tidytext’s cast_dtm().

```{r}
amzn_dtm <- tidy_amzn %>%
                count(productId, word, sort = TRUE) %>%
                ungroup() %>%
                cast_dtm(productId, word, n)

# 4 topics
product_lda <- LDA(amzn_dtm, k = 4, control = list(seed = 1234))

product_topics <- tidy(product_lda, matrix = "beta")

top_terms <- product_topics %>%
                group_by(topic) %>%
                top_n(5, beta) %>%
                ungroup() %>%
                arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()








```

